{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e1be2ada",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A API do Scopus encontrou 48 resultados.\n",
      " Carregando os resumos da busca...48 de 48"
     ]
    },
    {
     "data": {
      "text/plain": [
       "7     Comparison of two methods on vector space mode...\n",
       "18    Improved Collaborative Filtering Recommendatio...\n",
       "46    Adapting vector space model to ranking-based c...\n",
       "44    A hybrid content-based filtering approach: Rec...\n",
       "10    Cross-Content Recommendation between Movie and...\n",
       "Name: tituloArtigos, dtype: object"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#TRABALHO DE CONCLUSÃO\n",
    "\n",
    "import requests\n",
    "import pprint\n",
    "import urllib\n",
    "import json\n",
    "import time\n",
    "import pandas as pd\n",
    "import csv\n",
    "import re\n",
    "import numpy as np\n",
    "import nltk\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "#Constantes setadas para limpeza de texto dos resumos.\n",
    "PATTERN_S = re.compile(\"\\'s\")                # encontra os 's do texto \n",
    "PATTERN_RN = re.compile(\"\\\\r\\\\n\")            # encontra os \\r e \\n do texto\n",
    "PATTERN_PUNC = re.compile(r\"[^\\w\\s]\")        # encontra tudo que não é 0-9 A-z e ' '\n",
    "STOPWORDS = set(stopwords.words('english'))  # palavras comuns em inglês, como 'and, of, the, etc'\n",
    "\n",
    "#Seta URL para busca API. query é usada como teste\n",
    "url = 'https://api.elsevier.com/content/search/scopus'\n",
    "query = 'TF-IDF AND collaborative AND recommendation AND filtering AND similarity AND cosine AND mining AND NOT java'\n",
    "\n",
    "#Seta os parametros e headers para a busca API\n",
    "parameters = {\n",
    "    'query': urllib.parse.quote_plus(query),\n",
    "    'start': '0'\n",
    "}\n",
    "\n",
    "headers = {\n",
    "    'Accept': 'application/json',\n",
    "    'X-ELS-APIKey': 'f00003d0569e6d735211b697539544ea'\n",
    "}\n",
    "\n",
    "#Método usado para limpar o texto dos resumos\n",
    "def clean_text(text):\n",
    "    text = text.lower()  # lowercase text\n",
    "    # replace the matched string with ' '\n",
    "    text = re.sub(PATTERN_S, ' ', text)\n",
    "    text = re.sub(PATTERN_RN, ' ', text)\n",
    "    text = re.sub(PATTERN_PUNC, ' ', text)\n",
    "    return text\n",
    "\n",
    "#Método usado para remover as palavras que precedem NOT da query na vetorização\n",
    "def limpaNOTpalavra(query):\n",
    "    arr = query.split()\n",
    "    indices = []\n",
    "    for w in range(len(arr)):\n",
    "        if w > 0:\n",
    "            if arr[w-1] == 'NOT':\n",
    "                indices.insert(0, w)\n",
    "    for z in range(len(indices)):\n",
    "        arr.pop(indices[z])            \n",
    "    return ' '.join(arr)\n",
    "\n",
    "#Método usado para limpar os operadores da query antes de aplicar PLN\n",
    "def limpaOperadores(query):\n",
    "    query = limpaNOTpalavra(query)\n",
    "    palavras = ['AND', 'OR', 'NOT']\n",
    "    arr = query.split()\n",
    "    indices = []\n",
    "    for w in range(len(arr)):\n",
    "        for y in range(len(palavras)):\n",
    "            if palavras[y] == arr[w]:\n",
    "                indices.insert(0, w)\n",
    "    for z in range(len(indices)):\n",
    "        arr.pop(indices[z])\n",
    "    indices = []\n",
    "    return clean_text(' '.join(arr))\n",
    "    \n",
    "#Transforma a string resumo em lista de palavras, removendo plurais e padronizando variações\n",
    "def tokenizer(sentence, stopwords=STOPWORDS, lemmatize=True):\n",
    "    if lemmatize:\n",
    "        stemmer = WordNetLemmatizer()\n",
    "        tokens = [stemmer.lemmatize(w) for w in word_tokenize(sentence)]\n",
    "    else:\n",
    "        tokens = [w for w in word_tokenize(sentence)]\n",
    "    tokens = [w for w in tokens if w not in stopwords]\n",
    "    return tokens\n",
    "\n",
    "#Método para extrair os melhores índices após cálculo de semelhança\n",
    "def extract_best_indices(m, top):\n",
    "    # return the sum on all tokens of cosinus for each sentence\n",
    "    if len(m.shape) > 1:\n",
    "        cos_sim = np.mean(m, axis=0) \n",
    "    else: \n",
    "        cos_sim = m\n",
    "    index = np.argsort(cos_sim)[::-1] #organiza do índice de maior resultado ao menor\n",
    "    temp = np.ones(len(cos_sim))\n",
    "    temp = np.logical_or(cos_sim[index] != 0, temp) #elimina cosseno com distância 0\n",
    "    best_index = index[temp][:top]  \n",
    "    return best_index\n",
    "\n",
    "#Método principal; aplica a query na matriz, calcula semelhança e extrai os melhores resultados\n",
    "def get_recommendations_tfidf(sentence, tfidf_mat):\n",
    "    # Incorpora a query na matriz de vetores\n",
    "    tokens_query = [str(tok) for tok in tokenizer(sentence)]\n",
    "    embed_query = vectorizer.transform(tokens_query)\n",
    "    # Cria uma lista com similaridade entre a query e o dataset\n",
    "    mat = cosine_similarity(embed_query, tfidf_mat)\n",
    "    # Recebe os 5 índices de melhor similaridade de cosseno para cada token.\n",
    "    best_index = extract_best_indices(mat, top=5)\n",
    "    return best_index\n",
    "\n",
    "#Busca as URLs e títulos dos artigos retornados // Salva títulos em csv e retorna lista de URLs\n",
    "def buscaURL_Titulo(data):\n",
    "    listaURLS = []\n",
    "    with open('teste.csv', 'w', encoding='UTF8', newline='') as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow(['tituloArtigos'])\n",
    "        iter = int(data[\"search-results\"][\"opensearch:totalResults\"])\n",
    "        print(\"A API do Scopus encontrou \" + str(iter) + \" resultados.\")\n",
    "        if  iter > 25: #25 é o número de resultados que o jupyter mostra\n",
    "            if iter > 60:\n",
    "                iter = 60 #limitando a busca para no máximo 60 resultados visto que pode levar até 3min pra terminar a execução\n",
    "            for y in range(iter):\n",
    "                if ((y+1) % 26) == 0:\n",
    "                    parameters['start'] = str(int(parameters['start']) + 25)\n",
    "                    response = requests.get(url, params=parameters, headers=headers)\n",
    "                    data = response.json()\n",
    "                listaURLS.append(data['search-results']['entry'][y%25]['link'][2]['@href']) #URLs são usadas para obter os resumos\n",
    "                writer.writerow([data['search-results']['entry'][y%25]['dc:title']])        #Títulos são armazenados em .csv                \n",
    "        else:\n",
    "            for y in range(iter):\n",
    "                listaURLS.append(data['search-results']['entry'][y]['link'][2]['@href']) \n",
    "                writer.writerow([data['search-results']['entry'][y]['dc:title']]) \n",
    "    return listaURLS\n",
    "        \n",
    "#Busca o resumo para cada URL adquirida anteriormente // Retorna lista de resumos populada\n",
    "def buscaResumos(listaURLS):\n",
    "    #Inicializa a automação Selenium para busca de resumos\n",
    "    options = webdriver.ChromeOptions()\n",
    "    driver = webdriver.Chrome('chromedriver', options=options)\n",
    "    listaResumos = []\n",
    "    for y in range(len(listaURLS)):\n",
    "        robo = driver.get(listaURLS[y])\n",
    "        #Identifica se o elemento está presente na página\n",
    "        try:\n",
    "            element = WebDriverWait(driver, 5).until(\n",
    "            EC.presence_of_element_located((By.XPATH, '//*[@id=\"abstractSection\"]/p'))\n",
    "            )\n",
    "        except:\n",
    "            print(\"O resumo não foi encontrado na página.\")\n",
    "        time.sleep(1)\n",
    "        robo = driver.find_element(By.XPATH, '//*[@id=\"abstractSection\"]/p').text\n",
    "        #Caso o elemento não tenha sido corretamente adquirido, uma nova tentativa é feita após 3 segundos.\n",
    "        if robo == '':\n",
    "            time.sleep(3)\n",
    "            robo = driver.find_element(By.XPATH, '//*[@id=\"abstractSection\"]/p').text\n",
    "        listaResumos.append(clean_text(robo))\n",
    "        print(\"\\r\", \"Carregando os resumos da busca...\" + str(y+1) + \" de \" + str(len(listaURLS)), end=\"\")\n",
    "    driver.quit()\n",
    "    return listaResumos\n",
    "\n",
    "#Executa a busca API e transforma em json\n",
    "response = requests.get(url, params=parameters, headers=headers)\n",
    "data = response.json()\n",
    "resumos = buscaResumos(buscaURL_Titulo(data))\n",
    "\n",
    "# #Lê o .csv e adiciona os resumos aos títulos\n",
    "tabela = pd.read_csv('teste.csv')\n",
    "tabela['resumos'] = resumos\n",
    "\n",
    "#Aplica os stopwords na vetorização da matriz\n",
    "token_stop = tokenizer(' '.join(STOPWORDS), lemmatize=False)\n",
    "vectorizer = TfidfVectorizer(stop_words=token_stop, tokenizer=tokenizer) \n",
    "\n",
    "#Constrói a matriz vetorizada dos resumos, matriz de tamanho numero-de-palavras x numero-do-vocabulario\n",
    "tfidf_mat = vectorizer.fit_transform(tabela['resumos'].values)\n",
    "\n",
    "#Pega os melhores 5* artigos com base na query providenciada (numero de artigos retornados pode alterar)\n",
    "best_index = get_recommendations_tfidf(limpaOperadores(query), tfidf_mat)\n",
    "\n",
    "#Mostra o título dos melhores artigos\n",
    "display(tabela['tituloArtigos'].iloc[best_index])\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
